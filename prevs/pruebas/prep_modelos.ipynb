{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Carga de datos y division\n"
      ],
      "metadata": {
        "id": "D8pWEIY5Wfsb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEzq5K_xWNiw",
        "outputId": "77ba33de-d413-4ff4-ac4a-47e0ffa28e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos originales:\n",
            "sentimiento\n",
            "negativo    1200\n",
            "positivo     800\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Datos añadidos:\n",
            "sentimiento\n",
            "positivo    400\n",
            "negativo      0\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribución final de los datos:\n",
            "sentimiento\n",
            "positivo    1200\n",
            "negativo    1200\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Muestra de datos equilibrados:\n",
            "                                      reviews.text_esp  reviews.rating  \\\n",
            "102  La tableta fue excelente, el rendimiento y el ...             5.0   \n",
            "435  Me gusta recomendar este producto para futuros...             4.0   \n",
            "270  Atenuación de luces manos libres. Música a la ...             5.0   \n",
            "106  Compré esta tableta para mis hijos, me encanta...             5.0   \n",
            "71   ¡Gran dispositivo! Respuesta rápida con contro...             5.0   \n",
            "\n",
            "    sentimiento  \n",
            "102    positivo  \n",
            "435    positivo  \n",
            "270    positivo  \n",
            "106    positivo  \n",
            "71     positivo  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Cargar datos\n",
        "url = \"https://raw.githubusercontent.com/ignaciomsarmiento/RecomSystemsLectures/main/L07_sentimientos/data/Amazon.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Verificar las columnas y una muestra de los datos\n",
        "required_columns = ['reviews.text_esp', 'reviews.rating']\n",
        "missing_columns = [col for col in required_columns if col not in data.columns]\n",
        "if missing_columns:\n",
        "    raise ValueError(f\"Las siguientes columnas necesarias están ausentes en los datos: {missing_columns}\")\n",
        "\n",
        "data = data[['reviews.text_esp', 'reviews.rating']].copy()\n",
        "\n",
        "# Asignar sentimiento: >= 4.0 es positivo, < 4.0 es negativo\n",
        "data['sentimiento'] = data['reviews.rating'].apply(lambda x: 'positivo' if x >= 4.0 else 'negativo')\n",
        "\n",
        "# Resumir los datos originales\n",
        "original_counts = data['sentimiento'].value_counts()\n",
        "print(\"Datos originales:\")\n",
        "print(original_counts)\n",
        "\n",
        "# Separar datos positivos y negativos\n",
        "positivos = data[data['sentimiento'] == 'positivo']\n",
        "negativos = data[data['sentimiento'] == 'negativo']\n",
        "\n",
        "# Balancear duplicando los datos de la clase minoritaria\n",
        "if len(positivos) > len(negativos):\n",
        "    negativos = resample(negativos, replace=True, n_samples=len(positivos), random_state=42)\n",
        "else:\n",
        "    positivos = resample(positivos, replace=True, n_samples=len(negativos), random_state=42)\n",
        "\n",
        "# Concatenar los datos equilibrados\n",
        "data = pd.concat([positivos, negativos])\n",
        "\n",
        "# Resumir los datos finales\n",
        "#final_counts = data['sentimiento'].value_counts()\n",
        "#added_data = final_counts - original_counts.reindex(final_counts.index, fill_value=0)\n",
        "\n",
        "# Mostrar resumen\n",
        "print(\"\\nDatos añadidos:\")\n",
        "print(added_data)\n",
        "\n",
        "#print(\"\\nDistribución final de los datos:\")\n",
        "#print(final_counts)\n",
        "\n",
        "# Mostrar una muestra de los datos procesados\n",
        "print(\"\\nMuestra de datos equilibrados:\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento y generacion de embeddings\n"
      ],
      "metadata": {
        "id": "D3r6-spEXXZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words\n",
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from multiprocessing import Pool\n",
        "import math\n",
        "import spacy\n",
        "from num2words import num2words\n",
        "\n",
        "\n",
        "# Cargar modelo de Spacy para el preprocesamiento\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Definir stop words adicionales\n",
        "stop_words_adicionales = {\"\\u00a1\", \"-\", \"\\u2014\", \"http\", \"<\", \">\"}\n",
        "for palabra in stop_words_adicionales:\n",
        "    nlp.Defaults.stop_words.add(palabra)\n",
        "\n",
        "# Función para procesar texto\n",
        "def procesar_texto(texto):\n",
        "    oraciones = texto.split('\\n')\n",
        "    total_oraciones = len(oraciones)\n",
        "    oraciones_tokenizadas = []\n",
        "\n",
        "    for oracion in oraciones:\n",
        "        tokens = []\n",
        "        doc = nlp(oracion)\n",
        "        for token in doc:\n",
        "            if token.is_digit:\n",
        "                try:\n",
        "                    numero = int(token.text)\n",
        "                    palabra_letras = num2words(numero, lang='es')\n",
        "                    tokens.append(palabra_letras)\n",
        "                except ValueError:\n",
        "                    pass\n",
        "            else:\n",
        "                lemma = token.text.lower()\n",
        "                if lemma and lemma not in stop_words_adicionales:\n",
        "                    tokens.append(lemma)\n",
        "        oraciones_tokenizadas.append(\" \".join(tokens))\n",
        "\n",
        "    return total_oraciones, oraciones_tokenizadas\n",
        "\n",
        "# Paralelizar el preprocesamiento del texto\n",
        "def procesar_textos_en_paralelo(data_slice):\n",
        "    resultados = [procesar_texto(texto) for texto in data_slice['reviews.text_esp']]\n",
        "    total_oraciones, oraciones_tokenizadas = zip(*resultados)\n",
        "    return list(total_oraciones), list(oraciones_tokenizadas)\n",
        "\n",
        "# Inicializar el modelo BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
        "model = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\", ignore_mismatched_sizes=True)\n",
        "\n",
        "# Función para obtener el embedding de una oración\n",
        "def obtener_embedding_oracion(oracion):\n",
        "    inputs = tokenizer(oracion, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# Función para obtener el embedding promedio de una reseña completa\n",
        "def obtener_embedding_resena(oraciones_tokenizadas):\n",
        "    embeddings_oraciones = [obtener_embedding_oracion(oracion) for oracion in oraciones_tokenizadas]\n",
        "    return np.mean(embeddings_oraciones, axis=0) if embeddings_oraciones else np.zeros(model.config.hidden_size)\n",
        "\n",
        "# Paralelizar la generación de embeddings\n",
        "def procesar_rango(data_slice):\n",
        "    embeddings = [obtener_embedding_resena(oraciones) for oraciones in data_slice['oraciones_tokenizadas']]\n",
        "    return embeddings\n",
        "\n",
        "# Función para dividir los datos en subconjuntos para procesamiento paralelo\n",
        "def dividir_datos(data, n_cores):\n",
        "    longitud = len(data)\n",
        "    p_mas = len(data) % n_cores\n",
        "    numero_datos = math.floor(longitud / n_cores)\n",
        "    limites = [(i * numero_datos + min(i, p_mas), (i + 1) * numero_datos + min(i + 1, p_mas)) for i in range(n_cores)]\n",
        "    return limites\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    n_cores = 7\n",
        "\n",
        "\n",
        "    # Dividir datos para preprocesamiento\n",
        "    limites = dividir_datos(data, n_cores)\n",
        "    data_slices = [data.iloc[lim_inf:lim_sup] for lim_inf, lim_sup in limites]\n",
        "\n",
        "    # Preprocesar textos en paralelo\n",
        "    with Pool(n_cores) as pool:\n",
        "        resultados_preprocesamiento = pool.map(procesar_textos_en_paralelo, data_slices)\n",
        "\n",
        "    # Consolidar resultados del preprocesamiento\n",
        "    total_oraciones = []\n",
        "    oraciones_tokenizadas = []\n",
        "    for total, tokenizadas in resultados_preprocesamiento:\n",
        "        total_oraciones.extend(total)\n",
        "        oraciones_tokenizadas.extend(tokenizadas)\n",
        "\n",
        "    data['total_oraciones'] = total_oraciones\n",
        "    data['oraciones_tokenizadas'] = oraciones_tokenizadas\n",
        "\n",
        "    # Dividir datos para generación de embeddings\n",
        "    limites = dividir_datos(data, n_cores)\n",
        "    data_slices = [data.iloc[lim_inf:lim_sup] for lim_inf, lim_sup in limites]\n",
        "\n",
        "    # Generar embeddings en paralelo\n",
        "    with Pool(n_cores) as pool:\n",
        "        resultados_embeddings = pool.map(procesar_rango, data_slices)\n",
        "\n",
        "    # Consolidar resultados de embeddings\n",
        "    embeddings = [embedding for resultado in resultados_embeddings for embedding in resultado]\n",
        "    data['embedding_resena'] = embeddings\n",
        "\n",
        "    # Convertir embeddings a DataFrame y guardar con etiquetas\n",
        "    embeddings_df = pd.DataFrame(data['embedding_resena'].to_list())\n",
        "    embeddings_df['sentimiento'] = data['sentimiento'].values\n",
        "    embeddings_df.to_csv('embeddings_con_etiqueta.csv', index=False)\n",
        "\n",
        "    print(\"Embeddings por reseña con etiquetas guardados en 'embeddings_con_etiqueta.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PNnSM_BWvNk",
        "outputId": "3cd5af8b-6c95-4b5e-8de0-5f19a48794c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (0.5.14)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words) (0.6.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings por reseña con etiquetas guardados en 'embeddings_con_etiqueta.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grid search, para definir mejores hiperparametros en el posterior paso\n"
      ],
      "metadata": {
        "id": "OZfhYKpRa4lH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 01/ 01/ 2025\n",
        "# este programa realiza una busqued a de hiperparametros para seleccionar la que\n",
        "# la que arroje mejor precision esto se hace en paralelo combinando diferentes hiperparametros\n",
        "# este paso con el fin de elegir los mejores hiperparametros para la clasifciacion posterior a eswte paso\n",
        "#!pip install multiprocess\n",
        "\n",
        "import itertools\n",
        "import multiprocess\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "n_cores = 4\n",
        "\n",
        "def Nivelacion_de_Cargas(n_cores, lista_inicial):\n",
        "    lista_final = []\n",
        "    longitud_li = len(lista_inicial)\n",
        "    carga = longitud_li // n_cores\n",
        "    salidas = longitud_li % n_cores\n",
        "    contador = 0\n",
        "\n",
        "    for i in range(n_cores):\n",
        "        if i < salidas:\n",
        "            carga2 = contador + carga + 1\n",
        "        else:\n",
        "            carga2 = contador + carga\n",
        "        lista_final.append(lista_inicial[contador:carga2])\n",
        "        contador = carga2\n",
        "    return lista_final\n",
        "\n",
        "# Definir parámetros para SVM\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 0.001, 0.01]\n",
        "}\n",
        "\n",
        "keys_svm, values_svm = zip(*param_grid_svm.items())\n",
        "combinations_svm = [dict(zip(keys_svm, v)) for v in itertools.product(*values_svm)]\n",
        "\n",
        "# Definir parámetros para MLPClassifier (Red Neuronal)\n",
        "param_grid_rn = {\n",
        "    'hidden_layer_sizes': [(50,), (100,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['adam', 'sgd']\n",
        "}\n",
        "\n",
        "keys_rn, values_rn = zip(*param_grid_rn.items())\n",
        "combinations_rn = [dict(zip(keys_rn, v)) for v in itertools.product(*values_rn)]\n",
        "\n",
        "# Evaluar conjunto de hiperparámetros para SVM\n",
        "def evaluate_svm(hyperparameter_set, mejor_result, lock):\n",
        "    df = pd.read_csv('embeddings_con_etiqueta.csv')\n",
        "    X = df.iloc[:, :-1].values\n",
        "    y = df.iloc[:, -1].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.20, random_state=42)\n",
        "\n",
        "    for s in hyperparameter_set:\n",
        "        clf = SVC()\n",
        "        clf.set_params(C=s['C'], kernel=s['kernel'], gamma=s['gamma'])\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        proce_accuracy = accuracy_score(y_test, y_pred)\n",
        "        proce_recall = recall_score(y_test, y_pred, pos_label=\"positivo\")\n",
        "\n",
        "        lock.acquire()\n",
        "        if proce_accuracy > mejor_result['accuracy']:\n",
        "            mejor_result['accuracy'] = proce_accuracy\n",
        "            mejor_result['recall'] = proce_recall\n",
        "            mejor_result['params'] = s\n",
        "        lock.release()\n",
        "\n",
        "# Evaluar conjunto de hiperparámetros para MLPClassifier\n",
        "def evaluate_rn(hyperparameter_set, mejor_result, lock):\n",
        "    df = pd.read_csv('embeddings_con_etiqueta.csv')\n",
        "    X = df.iloc[:, :-1].values\n",
        "    y = df.iloc[:, -1].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.20, random_state=42)\n",
        "\n",
        "    for s in hyperparameter_set:\n",
        "        clf = MLPClassifier()\n",
        "        clf.set_params(hidden_layer_sizes=s['hidden_layer_sizes'], activation=s['activation'], solver=s['solver'])\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = clf.predict(X_test)\n",
        "\n",
        "        proce_accuracy = accuracy_score(y_test, y_pred)\n",
        "        proce_recall = recall_score(y_test, y_pred, pos_label=\"positivo\")\n",
        "\n",
        "        lock.acquire()\n",
        "        if proce_accuracy > mejor_result['accuracy']:\n",
        "            mejor_result['accuracy'] = proce_accuracy\n",
        "            mejor_result['recall'] = proce_recall\n",
        "            mejor_result['params'] = s\n",
        "        lock.release()\n",
        "\n",
        "\n",
        "def evaluate_nb(mejor_result, lock):\n",
        "    df = pd.read_csv('embeddings_con_etiqueta.csv')\n",
        "    X = df.iloc[:, :-1].values\n",
        "    y = df.iloc[:, -1].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.20, random_state=42)\n",
        "\n",
        "    clf = GaussianNB()\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    proce_accuracy = accuracy_score(y_test, y_pred)\n",
        "    proce_recall = recall_score(y_test, y_pred, pos_label=\"positivo\")\n",
        "\n",
        "    lock.acquire()\n",
        "    if proce_accuracy > mejor_result['accuracy']:\n",
        "        mejor_result['accuracy'] = proce_accuracy\n",
        "        mejor_result['recall'] = proce_recall\n",
        "        mejor_result['params'] = None\n",
        "    lock.release()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    threads = []\n",
        "    lock = multiprocess.Lock()\n",
        "\n",
        "    with multiprocess.Manager() as manager:\n",
        "        mejor_result_svm = manager.dict({'accuracy': 0, 'recall': 0, 'params': None})\n",
        "        mejor_result_rn = manager.dict({'accuracy': 0, 'recall': 0, 'params': None})\n",
        "        mejor_result_nb = manager.dict({'accuracy': 0, 'recall': 0, 'params': None})\n",
        "\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        splits_svm = Nivelacion_de_Cargas(n_cores, combinations_svm)\n",
        "        splits_rn = Nivelacion_de_Cargas(n_cores, combinations_rn)\n",
        "\n",
        "        # Crear y ejecutar los procesos\n",
        "        for i in range(n_cores):\n",
        "            threads.append(multiprocess.Process(target=evaluate_svm, args=(splits_svm[i], mejor_result_svm, lock)))\n",
        "            threads.append(multiprocess.Process(target=evaluate_rn, args=(splits_rn[i], mejor_result_rn, lock)))\n",
        "\n",
        "        # Proceso para GaussianNB (sin splits)\n",
        "        threads.append(multiprocess.Process(target=evaluate_nb, args=(mejor_result_nb, lock)))\n",
        "\n",
        "        for thread in threads:\n",
        "            thread.start()\n",
        "        for thread in threads:\n",
        "            thread.join()\n",
        "\n",
        "        finish_time = time.perf_counter()\n",
        "\n",
        "        print(f\"\\nMejor accuracy SVM: {mejor_result_svm['accuracy']}, recall: {mejor_result_svm['recall']}, parámetros: {mejor_result_svm['params']}\")\n",
        "        print(f\"Mejor accuracy RN: {mejor_result_rn['accuracy']}, recall: {mejor_result_rn['recall']}, parámetros: {mejor_result_rn['params']}\")\n",
        "        print(f\"Mejor accuracy NB: {mejor_result_nb['accuracy']}, recall: {mejor_result_nb['recall']}\")\n",
        "\n",
        "        print(f\"\\nProgram finished in {finish_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBfmsPZMda3e",
        "outputId": "c14b749e-626e-4c13-ea57-35dc5bef3687"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mejor accuracy SVM: 0.9, recall: 0.9083333333333333, parámetros: {'C': 10, 'kernel': 'rbf', 'gamma': 0.01}\n",
            "Mejor accuracy RN: 0.9104166666666667, recall: 0.9416666666666667, parámetros: {'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'adam'}\n",
            "Mejor accuracy NB: 0.7729166666666667, recall: 0.6875\n",
            "\n",
            "Program finished in 225.60 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clasisifcicacion simultanea de modelos, utilizando el primer dataset etiquetado obtenido de la web"
      ],
      "metadata": {
        "id": "eHC-RFNhblgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from multiprocessing import Pool\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "# utiulizar los mejores hiperparametros dados por grid search previamnet\n",
        "def entrenar_modelo(tipo_modelo, x_train, y_train, x_test, y_test):\n",
        "    if tipo_modelo == 'SVM':\n",
        "        modelo = SVC(C=10, kernel='rbf', gamma =0.01)\n",
        "    elif tipo_modelo == 'NB':\n",
        "        modelo = GaussianNB()\n",
        "    elif tipo_modelo == 'RN':\n",
        "        modelo = MLPClassifier(hidden_layer_sizes=(50,),activation='relu', solver='adam',max_iter=300)\n",
        "    else:\n",
        "        raise ValueError(\"Tipo de modelo desconocido\")\n",
        "\n",
        "    modelo.fit(x_train, y_train)\n",
        "    y_pred = modelo.predict(x_test)\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return tipo_modelo, accuracy\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    archivo_csv = 'embeddings_con_etiqueta.csv'\n",
        "    datos = pd.read_csv(archivo_csv)\n",
        "\n",
        "    # Mapeo de etiquetas a valores binarios\n",
        "    datos['sentimiento_binario'] = datos['sentimiento'].map({'negativo': 0, 'positivo': 1})\n",
        "\n",
        "\n",
        "    X = datos.iloc[:, :-1].select_dtypes(include=['float', 'int']).values  # Convertir a matriz NumPy\n",
        "    y = datos['sentimiento_binario'].values\n",
        "\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Asegurar que los datos no sean solo de lectura\n",
        "    x_train, x_test = np.copy(x_train), np.copy(x_test)\n",
        "    y_train, y_test = np.copy(y_train), np.copy(y_test)\n",
        "\n",
        "\n",
        "    modelos = ['SVM', 'NB', 'RN']\n",
        "\n",
        "\n",
        "    inicio = time.time()\n",
        "\n",
        "    # Entrenamiento en paralelo\n",
        "    with Pool(processes=len(modelos)) as pool:\n",
        "        resultados = pool.starmap(entrenar_modelo, [(modelo, x_train, y_train, x_test, y_test) for modelo in modelos])\n",
        "\n",
        "    # Mostramos resultados\n",
        "    for modelo, accuracy in resultados:\n",
        "        print(f\"Modelo: {modelo}, Precisión: {accuracy:.2f}\")\n",
        "\n",
        "    print(f\"Tiempo total: {time.time() - inicio:.2f} segundos\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0aF6-CuhZYx",
        "outputId": "a6c10043-a83a-46aa-adc1-ce0987957100"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo: SVM, Precisión: 0.90\n",
            "Modelo: NB, Precisión: 0.77\n",
            "Modelo: RN, Precisión: 0.91\n",
            "Tiempo total: 16.74 segundos\n"
          ]
        }
      ]
    }
  ]
}