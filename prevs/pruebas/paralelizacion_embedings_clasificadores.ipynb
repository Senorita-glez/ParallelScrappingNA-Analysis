{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'id', 'reviews.text', 'reviews.rating',\n",
      "       'reviews.text_esp'],\n",
      "      dtype='object')\n",
      "   Unnamed: 0                    id  \\\n",
      "0           0  AVqVGWQDv8e3D1O-ldFr   \n",
      "1           1  AVqkIhwDv8e3D1O-lebb   \n",
      "2           2  AVphgVaX1cnluZ0-DR74   \n",
      "3           3  AVphgVaX1cnluZ0-DR74   \n",
      "4           4  AVqVGWLKnnc1JgDc3jF1   \n",
      "\n",
      "                                        reviews.text  reviews.rating  \\\n",
      "0  This is a very nice tablet for my GF who has n...             5.0   \n",
      "1  Love this tablet. Easy to use. And price was r...             5.0   \n",
      "2  Affordable price awesome quality I love my Ama...             5.0   \n",
      "3  I bought this after speaking with a sales rep ...             5.0   \n",
      "4  Bought this tablet for my 2 &1/2 year old and ...             5.0   \n",
      "\n",
      "                                    reviews.text_esp  \n",
      "0  Esta es una tableta muy agradable para mi novi...  \n",
      "1  Me encanta esta tableta. Fácil de usar. Y el p...  \n",
      "2  Precio asequible calidad increíble Me encanta ...  \n",
      "3  Compré esto después de hablar con un represent...  \n",
      "4  Compré esta tableta para mi hijo de 2 años y m...  \n",
      "                                    reviews.text_esp  reviews.rating  \\\n",
      "0  Esta es una tableta muy agradable para mi novi...             5.0   \n",
      "1  Me encanta esta tableta. Fácil de usar. Y el p...             5.0   \n",
      "2  Precio asequible calidad increíble Me encanta ...             5.0   \n",
      "3  Compré esto después de hablar con un represent...             5.0   \n",
      "4  Compré esta tableta para mi hijo de 2 años y m...             5.0   \n",
      "\n",
      "  sentimiento  \n",
      "0    positivo  \n",
      "1    positivo  \n",
      "2    positivo  \n",
      "3    positivo  \n",
      "4    positivo  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar datos desde la URL\n",
    "url = \"https://raw.githubusercontent.com/ignaciomsarmiento/RecomSystemsLectures/main/L07_sentimientos/data/Amazon.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Verificar las columnas y una muestra de los datos\n",
    "print(data.columns)\n",
    "print(data.head())\n",
    "\n",
    "# Asegurarse de que las columnas relevantes están disponibles\n",
    "required_columns = ['reviews.text_esp', 'reviews.rating']\n",
    "missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Las siguientes columnas necesarias están ausentes en los datos: {missing_columns}\")\n",
    "\n",
    "# Si necesitas procesar datos adicionales\n",
    "data = data[['reviews.text_esp', 'reviews.rating']].copy()\n",
    "\n",
    "# Agregar una columna de \"sentimiento\" basada en el valor de 'reviews.rating' (opcional)\n",
    "# Por ejemplo, suponiendo: >= 4.0 es positivo, < 4.0 es negativo\n",
    "data['sentimiento'] = data['reviews.rating'].apply(lambda x: 'positivo' if x >= 4.0 else 'negativo')\n",
    "\n",
    "# Mostrar una muestra procesada\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from multiprocessing import Pool\n",
    "import math\n",
    "import spacy\n",
    "from num2words import num2words\n",
    "\n",
    "# Cargar modelo de Spacy para el preprocesamiento\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Definir stop words adicionales\n",
    "stop_words_adicionales = {\"\\u00a1\", \"-\", \"\\u2014\", \"http\", \"<\", \">\"}\n",
    "for palabra in stop_words_adicionales:\n",
    "    nlp.Defaults.stop_words.add(palabra)\n",
    "\n",
    "# Función para procesar texto\n",
    "\n",
    "def procesar_texto(texto):\n",
    "    oraciones = texto.split('\\n')\n",
    "    total_oraciones = len(oraciones)\n",
    "    oraciones_tokenizadas = []\n",
    "\n",
    "    for oracion in oraciones:\n",
    "        tokens = []\n",
    "        doc = nlp(oracion)\n",
    "        for token in doc:\n",
    "            if token.is_digit:\n",
    "                try:\n",
    "                    numero = int(token.text)\n",
    "                    palabra_letras = num2words(numero, lang='es')\n",
    "                    tokens.append(palabra_letras)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            else:\n",
    "                lemma = token.text.lower()\n",
    "                if lemma and lemma not in stop_words_adicionales:\n",
    "                    tokens.append(lemma)\n",
    "        oraciones_tokenizadas.append(\" \".join(tokens))\n",
    "\n",
    "    return total_oraciones, oraciones_tokenizadas\n",
    "\n",
    "# Paralelizar el preprocesamiento del texto\n",
    "def procesar_textos_en_paralelo(data_slice):\n",
    "    resultados = [procesar_texto(texto) for texto in data_slice['reviews.text_esp']]\n",
    "    total_oraciones, oraciones_tokenizadas = zip(*resultados)\n",
    "    return total_oraciones, oraciones_tokenizadas\n",
    "\n",
    "# Inicializar el modelo BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
    "model = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\", ignore_mismatched_sizes=True)\n",
    "\n",
    "# Función para obtener el embeddin\n",
    "def obtener_embedding_oracion(oracion):\n",
    "    inputs = tokenizer(oracion, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Función para obtener el embedding promedio de una reseña completa\n",
    "def obtener_embedding_resena(oraciones_tokenizadas):\n",
    "    embeddings_oraciones = [obtener_embedding_oracion(oracion) for oracion in oraciones_tokenizadas]\n",
    "    return np.mean(embeddings_oraciones, axis=0) if embeddings_oraciones else np.zeros(model.config.hidden_size)\n",
    "\n",
    "# Paralelizar la generación de embeddings\n",
    "def procesar_rango(data_slice):\n",
    "    embeddings = [obtener_embedding_resena(oraciones) for oraciones in data_slice['oraciones_tokenizadas']]\n",
    "    return embeddings\n",
    "\n",
    "# Función para dividir los datos en subconjuntos para procesamiento paralelo\n",
    "def dividir_datos(data, n_cores):\n",
    "    longitud = len(data)\n",
    "    p_mas = len(data) % n_cores\n",
    "    numero_datos = math.floor(longitud / n_cores)\n",
    "    limites = [(i * numero_datos + min(i, p_mas), (i + 1) * numero_datos + min(i + 1, p_mas)) for i in range(n_cores)]\n",
    "    return limites\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Cargar datos (asumiendo que ya están en un DataFrame llamado 'data')\n",
    "    # data = pd.read_csv('data.csv')\n",
    "\n",
    "    n_cores = 7\n",
    "\n",
    "    # Dividir datos para preprocesamiento\n",
    "    limites = dividir_datos(data, n_cores)\n",
    "    data_slices = [data.iloc[lim_inf:lim_sup] for lim_inf, lim_sup in limites]\n",
    "\n",
    "    # Preprocesar textos en paralelo\n",
    "    with Pool(n_cores) as pool:\n",
    "        resultados_preprocesamiento = pool.map(procesar_textos_en_paralelo, data_slices)\n",
    "\n",
    "    # Consolidar resultados del preprocesamiento\n",
    "    total_oraciones = []\n",
    "    oraciones_tokenizadas = []\n",
    "    for total, tokenizadas in resultados_preprocesamiento:\n",
    "        total_oraciones.extend(total)\n",
    "        oraciones_tokenizadas.extend(tokenizadas)\n",
    "\n",
    "    data['total_oraciones'] = total_oraciones\n",
    "    data['oraciones_tokenizadas'] = oraciones_tokenizadas\n",
    "\n",
    "    # Dividir datos para generación de embeddings\n",
    "    limites = dividir_datos(data, n_cores)\n",
    "    data_slices = [data.iloc[lim_inf:lim_sup] for lim_inf, lim_sup in limites]\n",
    "\n",
    "    # Generar embeddings en paralelo\n",
    "    with Pool(n_cores) as pool:\n",
    "        resultados_embeddings = pool.map(procesar_rango, data_slices)\n",
    "\n",
    "    # Consolidar resultados de embeddings\n",
    "    embeddings = [embedding for resultado in resultados_embeddings for embedding in resultado]\n",
    "    data['embedding_resena'] = embeddings\n",
    "\n",
    "    # Convertir embeddings a DataFrame y guardar con etiquetas\n",
    "    embeddings_df = pd.DataFrame(data['embedding_resena'].to_list())\n",
    "    embeddings_df['sentimiento'] = data['sentimiento'].values\n",
    "    embeddings_df.to_csv('embeddings_con_etiqueta.csv', index=False)\n",
    "\n",
    "    print(\"Embeddings por reseña con etiquetas guardados en 'embeddings_con_etiqueta.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paralelizacion de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# utiulizar los mejores hiperparametros dados por grid search previamnet\n",
    "def entrenar_modelo(tipo_modelo, x_train, y_train, x_test, y_test):\n",
    "    if tipo_modelo == 'SVM':\n",
    "        modelo = SVC(C=10, kernel='rbf', gamma =0.01)\n",
    "    elif tipo_modelo == 'NB':\n",
    "        modelo = GaussianNB()\n",
    "    elif tipo_modelo == 'RN':\n",
    "        modelo = MLPClassifier(hidden_layer_sizes=(50,),activation='relu', solver='adam',max_iter=300)\n",
    "    else:\n",
    "        raise ValueError(\"Tipo de modelo desconocido\")\n",
    "    \n",
    "    modelo.fit(x_train, y_train)\n",
    "    y_pred = modelo.predict(x_test)\n",
    "\n",
    "  \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return tipo_modelo, accuracy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    archivo_csv = 'embeddings_con_etiqueta__.csv'\n",
    "    datos = pd.read_csv(archivo_csv)\n",
    "\n",
    "    # Mapeo de etiquetas a valores binarios\n",
    "    datos['sentimiento_binario'] = datos['sentimiento'].map({'NEGATIVO': 0, 'POSITIVO': 1})\n",
    "\n",
    "\n",
    "    X = datos.iloc[:, :-1].select_dtypes(include=['float', 'int']).values  # Convertir a matriz NumPy\n",
    "    y = datos['sentimiento_binario'].values  # Convertir a matriz NumPy\n",
    "\n",
    " \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Asegurar que los datos no sean solo de lectura\n",
    "    x_train, x_test = np.copy(x_train), np.copy(x_test)\n",
    "    y_train, y_test = np.copy(y_train), np.copy(y_test)\n",
    "\n",
    "\n",
    "    modelos = ['SVM', 'NB', 'RN']\n",
    "\n",
    "  \n",
    "    inicio = time.time()\n",
    "\n",
    "    # Entrenamiento en paralelo\n",
    "    with Pool(processes=len(modelos)) as pool:\n",
    "        resultados = pool.starmap(entrenar_modelo, [(modelo, x_train, y_train, x_test, y_test) for modelo in modelos])\n",
    "\n",
    "    # Mostramos resultados\n",
    "    for modelo, accuracy in resultados:\n",
    "        print(f\"Modelo: {modelo}, Precisión: {accuracy:.2f}\")\n",
    "\n",
    "    print(f\"Tiempo total: {time.time() - inicio:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Utilizar los mejores hiperparámetros dados por grid search previamente\n",
    "def entrenar_modelo(tipo_modelo, x_train, y_train, x_test, y_test):\n",
    "    if tipo_modelo == 'SVM':\n",
    "        modelo = SVC(C=10, kernel='rbf', gamma=0.01)\n",
    "    elif tipo_modelo == 'NB':\n",
    "        modelo = GaussianNB()\n",
    "    elif tipo_modelo == 'RN':\n",
    "        modelo = MLPClassifier(hidden_layer_sizes=(50,), activation='relu', solver='adam', max_iter=300)\n",
    "    elif tipo_modelo == 'KMeans':\n",
    "        # Definir el número de clusters igual a 2 (binario)\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "        kmeans.fit(x_train)\n",
    "\n",
    "        # Predecir los clusters para los datos de prueba\n",
    "        clusters_train = kmeans.predict(x_train)\n",
    "        clusters_test = kmeans.predict(x_test)\n",
    "\n",
    "        # Evaluar ambas configuraciones de asignación de etiquetas\n",
    "        accuracy_1 = calcular_precision(clusters_test, y_test, cluster_a=1, cluster_b=0)\n",
    "        accuracy_2 = calcular_precision(clusters_test, y_test, cluster_a=0, cluster_b=1)\n",
    "\n",
    "        # Escoger la mejor configuración\n",
    "        accuracy = max(accuracy_1, accuracy_2)\n",
    "        return tipo_modelo, accuracy\n",
    "    else:\n",
    "        raise ValueError(\"Tipo de modelo desconocido\")\n",
    "\n",
    "    modelo.fit(x_train, y_train)\n",
    "    y_pred = modelo.predict(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return tipo_modelo, accuracy\n",
    "\n",
    "def calcular_precision(clusters, y_true, cluster_a, cluster_b):\n",
    "    # Asignar etiquetas a los clusters\n",
    "    etiquetas_asignadas = np.where(clusters == cluster_a, 1, 0)\n",
    "    etiquetas_asignadas = np.where(clusters == cluster_b, 0, etiquetas_asignadas)\n",
    "\n",
    "    # Calcular precisión comparando con las etiquetas reales\n",
    "    return accuracy_score(y_true, etiquetas_asignadas)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    archivo_csv = 'embeddings_con_etiqueta__.csv'\n",
    "    datos = pd.read_csv(archivo_csv)\n",
    "\n",
    "    # Mapeo de etiquetas a valores binarios\n",
    "    datos['sentimiento_binario'] = datos['sentimiento'].map({'NEGATIVO': 0, 'POSITIVO': 1})\n",
    "\n",
    "    X = datos.iloc[:, :-1].select_dtypes(include=['float', 'int']).values  # Convertir a matriz NumPy\n",
    "    y = datos['sentimiento_binario'].values  # Convertir a matriz NumPy\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Asegurar que los datos no sean solo de lectura\n",
    "    x_train, x_test = np.copy(x_train), np.copy(x_test)\n",
    "    y_train, y_test = np.copy(y_train), np.copy(y_test)\n",
    "\n",
    "    # Agregar KMeans a la lista de modelos\n",
    "    modelos = ['SVM', 'NB', 'RN', 'KMeans']\n",
    "\n",
    "    inicio = time.time()\n",
    "\n",
    "    # Entrenamiento en paralelo\n",
    "    with Pool(processes=len(modelos)) as pool:\n",
    "        resultados = pool.starmap(entrenar_modelo, [(modelo, x_train, y_train, x_test, y_test) for modelo in modelos])\n",
    "\n",
    "    # Mostramos resultados\n",
    "    for modelo, accuracy in resultados:\n",
    "        print(f\"Modelo: {modelo}, Precisi\\u00f3n: {accuracy:.2f}\")\n",
    "\n",
    "    print(f\"Tiempo total: {time.time() - inicio:.2f} segundos\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
